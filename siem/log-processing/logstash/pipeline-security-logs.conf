# Logstash Pipeline Configuration for iSECTECH Security Logs
# Production-grade log processing with ECS compliance and custom format support
# Integrates with custom log parser and normalization engine

input {
  # Beats input for agent-based log collection
  beats {
    port => 5044
    type => "beats"
    ssl => true
    ssl_certificate => "/opt/logstash/config/certs/logstash.crt"
    ssl_key => "/opt/logstash/config/certs/logstash.key"
    ssl_verify_mode => "force_peer"
    ssl_peer_metadata => true
  }
  
  # Kafka input for high-volume streaming
  kafka {
    bootstrap_servers => "${KAFKA_BROKERS:localhost:9092}"
    topics => ["security-logs", "threat-intel", "audit-logs"]
    group_id => "logstash-security"
    consumer_threads => 4
    decorate_events => true
    security_protocol => "SSL"
    ssl_truststore_location => "/opt/logstash/config/certs/kafka.truststore.jks"
    ssl_truststore_password => "${KAFKA_TRUSTSTORE_PASSWORD}"
    ssl_keystore_location => "/opt/logstash/config/certs/kafka.keystore.jks"
    ssl_keystore_password => "${KAFKA_KEYSTORE_PASSWORD}"
    codec => json
    type => "kafka_security"
  }
  
  # Syslog input for network devices
  syslog {
    port => 514
    type => "syslog"
    use_labels => true
    facility_labels => ["kernel", "user", "mail", "daemon", "auth", "syslog", "lpr", "news", "uucp", "cron", "authpriv", "ftp", "local0", "local1", "local2", "local3", "local4", "local5", "local6", "local7"]
  }
  
  # TCP input for custom log sources
  tcp {
    port => 5000
    type => "tcp_raw"
    codec => json_lines
  }
  
  # HTTP input for webhooks and API logs
  http {
    port => 8080
    type => "http_webhook"
    ssl => true
    ssl_certificate => "/opt/logstash/config/certs/logstash.crt"
    ssl_key => "/opt/logstash/config/certs/logstash.key"
    codec => json
  }
}

filter {
  # Add processing metadata
  mutate {
    add_field => {
      "[@metadata][processing_timestamp]" => "%{+yyyy-MM-dd'T'HH:mm:ss.SSSZ}"
      "[@metadata][pipeline]" => "security-logs"
      "[@metadata][version]" => "1.0.0"
    }
  }
  
  # Drop empty or invalid events
  if ![message] and ![log] and ![event] {
    drop { }
  }
  
  # Handle different input types
  if [type] == "beats" {
    # Parse Beats metadata
    if [beat] {
      mutate {
        add_field => { 
          "[@metadata][beat_name]" => "%{[beat][name]}"
          "[@metadata][beat_version]" => "%{[beat][version]}"
        }
      }
    }
    
    # Extract log line
    if [message] {
      mutate { rename => { "message" => "[@metadata][raw_log]" } }
    }
  } else if [type] == "kafka_security" {
    # Handle Kafka events
    if [message] {
      mutate { rename => { "message" => "[@metadata][raw_log]" } }
    }
  } else if [type] == "syslog" {
    # Handle syslog events
    mutate { 
      add_field => { "[@metadata][raw_log]" => "%{message}" }
      add_field => { "[@metadata][format]" => "syslog" }
    }
  } else if [type] == "tcp_raw" {
    # Handle raw TCP logs
    if [message] {
      mutate { rename => { "message" => "[@metadata][raw_log]" } }
    }
  } else if [type] == "http_webhook" {
    # Handle HTTP webhook events
    mutate { 
      add_field => { "[@metadata][raw_log]" => "%{message}" }
      add_field => { "[@metadata][format]" => "json" }
    }
  }
  
  # Auto-detect log format if not specified
  if ![@metadata][format] {
    if [@metadata][raw_log] =~ /^\{.*\}$/ {
      mutate { add_field => { "[@metadata][format]" => "json" } }
    } else if [@metadata][raw_log] =~ /^<\d+>/ {
      mutate { add_field => { "[@metadata][format]" => "syslog" } }
    } else if [@metadata][raw_log] =~ /^\S+ \S+ \S+ \[[\d\/\w:+\s]+\] "\w+ \S+ HTTP/ {
      mutate { add_field => { "[@metadata][format]" => "apache" } }
    } else if [@metadata][raw_log] =~ /.*,.*,.*/ {
      mutate { add_field => { "[@metadata][format]" => "csv" } }
    } else {
      mutate { add_field => { "[@metadata][format]" => "unknown" } }
    }
  }
  
  # Parse based on detected format
  if [@metadata][format] == "json" {
    json {
      source => "[@metadata][raw_log]"
      target => "parsed"
      skip_on_invalid_json => true
      tag_on_failure => ["_jsonparsefailure"]
    }
    
    # Flatten parsed JSON
    if [parsed] {
      ruby {
        code => "
          def flatten_hash(hash, parent_key = '', sep = '.')
            hash.each_with_object({}) do |(k, v), h|
              new_key = parent_key.empty? ? k.to_s : [parent_key, k].join(sep)
              if v.is_a?(Hash)
                h.merge!(flatten_hash(v, new_key, sep))
              else
                h[new_key] = v
              end
            end
          end
          
          if event.get('parsed')
            flattened = flatten_hash(event.get('parsed'))
            flattened.each { |k, v| event.set(k, v) }
            event.remove('parsed')
          end
        "
      }
    }
  } else if [@metadata][format] == "syslog" {
    grok {
      match => { 
        "[@metadata][raw_log]" => [
          "<%{POSINT:syslog_pri}>%{SYSLOGTIMESTAMP:syslog_timestamp} %{IPORHOST:syslog_server} %{PROG:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}",
          "<%{POSINT:syslog_pri}>%{TIMESTAMP_ISO8601:syslog_timestamp} %{IPORHOST:syslog_server} %{PROG:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}"
        ]
      }
      tag_on_failure => ["_grokparsefailure_syslog"]
    }
    
    # Calculate syslog facility and severity
    if [syslog_pri] {
      ruby {
        code => "
          pri = event.get('syslog_pri').to_i
          event.set('syslog_facility', pri >> 3)
          event.set('syslog_severity', pri & 7)
        "
      }
    }
    
    # Parse timestamp
    if [syslog_timestamp] {
      date {
        match => [ "syslog_timestamp", "MMM dd HH:mm:ss", "MMM  d HH:mm:ss", "ISO8601" ]
        target => "@timestamp"
      }
    }
  } else if [@metadata][format] == "apache" {
    grok {
      match => { 
        "[@metadata][raw_log]" => "%{COMBINEDAPACHELOG}"
      }
      tag_on_failure => ["_grokparsefailure_apache"]
    }
    
    # Parse timestamp
    if [timestamp] {
      date {
        match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
        target => "@timestamp"
      }
    }
  }
  
  # Apply custom format parsing for known security tools
  if [agent] and [agent][name] {
    # CrowdStrike Falcon
    if [agent][name] =~ /(?i)crowdstrike|falcon/ {
      if [event_simpleName] {
        mutate {
          add_field => { "event.action" => "%{event_simpleName}" }
          add_field => { "event.provider" => "crowdstrike_falcon" }
          add_field => { "[@metadata][custom_format]" => "crowdstrike_falcon" }
        }
      }
      
      # Map CrowdStrike fields to ECS
      if [ComputerName] { mutate { add_field => { "host.name" => "%{ComputerName}" } } }
      if [UserName] { mutate { add_field => { "user.name" => "%{UserName}" } } }
      if [ProcessId] { mutate { add_field => { "process.pid" => "%{ProcessId}" } } }
      if [ParentProcessId] { mutate { add_field => { "process.parent.pid" => "%{ParentProcessId}" } } }
      if [ImageFileName] { mutate { add_field => { "process.name" => "%{ImageFileName}" } } }
      if [CommandLine] { mutate { add_field => { "process.command_line" => "%{CommandLine}" } } }
      if [MD5HashData] { mutate { add_field => { "process.hash.md5" => "%{MD5HashData}" } } }
      if [SHA256HashData] { mutate { add_field => { "process.hash.sha256" => "%{SHA256HashData}" } } }
      if [LocalAddressIP4] { mutate { add_field => { "source.ip" => "%{LocalAddressIP4}" } } }
      if [RemoteAddressIP4] { mutate { add_field => { "destination.ip" => "%{RemoteAddressIP4}" } } }
      if [LocalPort] { mutate { add_field => { "source.port" => "%{LocalPort}" } } }
      if [RemotePort] { mutate { add_field => { "destination.port" => "%{RemotePort}" } } }
      if [DetectName] { mutate { add_field => { "rule.name" => "%{DetectName}" } } }
      if [Severity] { mutate { add_field => { "event.severity" => "%{Severity}" } } }
      if [Tactic] { mutate { add_field => { "threat.tactic.name" => "%{Tactic}" } } }
      if [Technique] { mutate { add_field => { "threat.technique.name" => "%{Technique}" } } }
    }
    
    # Microsoft Defender ATP
    else if [agent][name] =~ /(?i)microsoft|defender|winlogbeat/ {
      if [ActionType] {
        mutate {
          add_field => { "event.action" => "%{ActionType}" }
          add_field => { "event.provider" => "microsoft_defender" }
          add_field => { "[@metadata][custom_format]" => "microsoft_defender_atp" }
        }
      }
      
      # Map Defender fields to ECS
      if [DeviceName] { mutate { add_field => { "host.name" => "%{DeviceName}" } } }
      if [AccountName] { mutate { add_field => { "user.name" => "%{AccountName}" } } }
      if [ProcessCommandLine] { mutate { add_field => { "process.command_line" => "%{ProcessCommandLine}" } } }
      if [FileName] { mutate { add_field => { "file.name" => "%{FileName}" } } }
      if [SHA256] { mutate { add_field => { "file.hash.sha256" => "%{SHA256}" } } }
      if [LocalIP] { mutate { add_field => { "source.ip" => "%{LocalIP}" } } }
      if [RemoteIP] { mutate { add_field => { "destination.ip" => "%{RemoteIP}" } } }
      if [ThreatFamily] { mutate { add_field => { "threat.technique.name" => "%{ThreatFamily}" } } }
    }
    
    # Okta System Logs
    else if [agent][name] =~ /(?i)okta/ {
      if [eventType] {
        mutate {
          add_field => { "event.action" => "%{eventType}" }
          add_field => { "event.provider" => "okta" }
          add_field => { "[@metadata][custom_format]" => "okta_system_log" }
        }
      }
      
      # Map Okta fields to ECS
      if [actor][alternateId] { mutate { add_field => { "user.email" => "%{[actor][alternateId]}" } } }
      if [actor][displayName] { mutate { add_field => { "user.full_name" => "%{[actor][displayName]}" } } }
      if [client][ipAddress] { mutate { add_field => { "source.ip" => "%{[client][ipAddress]}" } } }
      if [client][userAgent][rawUserAgent] { mutate { add_field => { "user_agent.original" => "%{[client][userAgent][rawUserAgent]}" } } }
      if [outcome][result] { mutate { add_field => { "event.outcome" => "%{[outcome][result]}" } } }
    }
  }
  
  # Palo Alto Networks threat logs
  if [@metadata][raw_log] =~ /^[^,]*,[^,]*,THREAT/ {
    csv {
      source => "[@metadata][raw_log]"
      columns => [
        "receive_time", "serial", "type", "threat_content_type", "config_version",
        "generate_time", "source_address", "destination_address", "nat_source_ip",
        "nat_destination_ip", "rule_name", "source_user", "destination_user",
        "application", "virtual_system", "source_zone", "destination_zone",
        "inbound_interface", "outbound_interface", "log_action", "time_logged",
        "session_id", "repeat_count", "source_port", "destination_port",
        "nat_source_port", "nat_destination_port", "flags", "protocol", "action",
        "misc", "threat_id", "category", "severity", "direction", "sequence_number",
        "action_flags", "source_country", "destination_country", "content_type",
        "pcap_id", "file_digest", "cloud", "url_index", "user_agent", "file_type"
      ]
      skip_empty_columns => true
      tag_on_failure => ["_csvparsefailure_paloalto"]
    }
    
    mutate {
      add_field => { "event.provider" => "palo_alto_networks" }
      add_field => { "[@metadata][custom_format]" => "palo_alto_threat" }
    }
    
    # Map PAN fields to ECS
    if [source_address] { mutate { rename => { "source_address" => "source.ip" } } }
    if [destination_address] { mutate { rename => { "destination_address" => "destination.ip" } } }
    if [source_port] { mutate { rename => { "source_port" => "source.port" } } }
    if [destination_port] { mutate { rename => { "destination_port" => "destination.port" } } }
    if [rule_name] { mutate { rename => { "rule_name" => "rule.name" } } }
    if [threat_id] { mutate { rename => { "threat_id" => "threat.technique.id" } } }
    if [category] { mutate { rename => { "category" => "threat.tactic.name" } } }
    if [action] { mutate { rename => { "action" => "event.action" } } }
  }
  
  # Normalize severity values
  if [event][severity] {
    if [event][severity] =~ /(?i)critical|fatal/ {
      mutate { replace => { "[event][severity]" => 90 } }
    } else if [event][severity] =~ /(?i)high|error/ {
      mutate { replace => { "[event][severity]" => 70 } }
    } else if [event][severity] =~ /(?i)medium|warning/ {
      mutate { replace => { "[event][severity]" => 50 } }
    } else if [event][severity] =~ /(?i)low/ {
      mutate { replace => { "[event][severity]" => 30 } }
    } else if [event][severity] =~ /(?i)info|informational/ {
      mutate { replace => { "[event][severity]" => 10 } }
    }
  }
  
  # Normalize event outcome
  if [event][outcome] {
    if [event][outcome] =~ /(?i)success|allowed|pass/ {
      mutate { replace => { "[event][outcome]" => "success" } }
    } else if [event][outcome] =~ /(?i)fail|failure|denied|blocked|error/ {
      mutate { replace => { "[event][outcome]" => "failure" } }
    } else {
      mutate { replace => { "[event][outcome]" => "unknown" } }
    }
  }
  
  # Add ECS compliance fields
  mutate {
    add_field => { "ecs.version" => "8.11.0" }
  }
  
  # Ensure @timestamp exists
  if ![@timestamp] {
    mutate { add_field => { "@timestamp" => "%{+yyyy-MM-dd'T'HH:mm:ss.SSSZ}" } }
  }
  
  # Add iSECTECH specific labels
  mutate {
    add_field => {
      "labels.isectech_tenant_id" => "${ISECTECH_TENANT_ID:default}"
      "labels.isectech_environment" => "${ISECTECH_ENVIRONMENT:production}"
      "labels.isectech_pipeline" => "security-logs"
    }
  }
  
  # GeoIP enrichment for IP addresses
  if [source][ip] {
    geoip {
      source => "[source][ip]"
      target => "[source][geo]"
      database => "/opt/logstash/vendor/geoip/GeoLite2-City.mmdb"
      tag_on_failure => ["_geoip_lookup_failure"]
    }
  }
  
  if [destination][ip] {
    geoip {
      source => "[destination][ip]"
      target => "[destination][geo]"
      database => "/opt/logstash/vendor/geoip/GeoLite2-City.mmdb"
      tag_on_failure => ["_geoip_lookup_failure"]
    }
  }
  
  # User agent parsing
  if [user_agent][original] {
    useragent {
      source => "[user_agent][original]"
      target => "[user_agent]"
    }
  }
  
  # Generate event fingerprint
  fingerprint {
    source => ["@timestamp", "[source][ip]", "[destination][ip]", "[event][action]", "[host][name]"]
    target => "[@metadata][fingerprint]"
    method => "SHA256"
    base64encode => true
  }
  
  # Add fingerprint to event
  mutate {
    add_field => { "event.hash" => "%{[@metadata][fingerprint]}" }
  }
  
  # Calculate risk score based on various factors
  ruby {
    code => "
      risk_score = 0
      
      # Severity contribution (0-40 points)
      severity = event.get('[event][severity]')
      if severity
        risk_score += [severity.to_i * 0.4, 40].min
      end
      
      # Threat intelligence contribution (0-30 points)
      if event.get('[threat][technique][name]')
        risk_score += 20
      end
      if event.get('[threat][tactic][name]')
        risk_score += 10
      end
      
      # Source reputation contribution (0-20 points)
      source_ip = event.get('[source][ip]')
      if source_ip
        # Check for private IP ranges (lower risk)
        if source_ip.match(/^(10\.|172\.(1[6-9]|2[0-9]|3[01])\.|192\.168\.)/)
          # Private IP, lower additional risk
          risk_score += 5
        else
          # Public IP, higher risk
          risk_score += 15
        end
      end
      
      # Event outcome contribution (0-10 points)
      outcome = event.get('[event][outcome]')
      if outcome == 'failure'
        risk_score += 10
      elsif outcome == 'success'
        risk_score += 3
      end
      
      event.set('[event][risk_score]', [risk_score, 100].min)
    "
  }
  
  # Remove temporary and metadata fields
  mutate {
    remove_field => [
      "[@metadata][raw_log]",
      "[@metadata][format]",
      "[@metadata][processing_timestamp]",
      "beat",
      "prospector",
      "input",
      "offset",
      "source"
    ]
  }
  
  # Drop events that failed critical parsing
  if "_jsonparsefailure" in [tags] or "_grokparsefailure_syslog" in [tags] {
    if [@metadata][drop_on_parse_failure] == "true" {
      drop { }
    }
  }
}

output {
  # Primary output to Elasticsearch
  elasticsearch {
    hosts => ["${ELASTICSEARCH_HOSTS:localhost:9200}"]
    index => "isectech-security-logs-%{+yyyy.MM.dd}"
    template_name => "isectech-security-logs"
    template => "/opt/logstash/templates/isectech-security-logs.json"
    template_overwrite => true
    
    # Security configuration
    ssl => true
    ssl_certificate_verification => true
    cacert => "/opt/logstash/config/certs/ca.crt"
    user => "${ELASTICSEARCH_USERNAME}"
    password => "${ELASTICSEARCH_PASSWORD}"
    
    # Performance tuning
    workers => 4
    flush_size => 1000
    idle_flush_time => 1
    
    # Document ID using fingerprint for deduplication
    document_id => "%{[@metadata][fingerprint]}"
    
    # ILM policy
    ilm_enabled => true
    ilm_rollover_alias => "isectech-security-logs"
    ilm_pattern => "{now/d}-000001"
    ilm_policy => "isectech-security-logs-policy"
  }
  
  # High-priority alerts to Kafka for real-time processing
  if [event][risk_score] and [event][risk_score] >= 70 {
    kafka {
      bootstrap_servers => "${KAFKA_BROKERS:localhost:9092}"
      topic_id => "high-priority-alerts"
      compression_type => "snappy"
      security_protocol => "SSL"
      ssl_truststore_location => "/opt/logstash/config/certs/kafka.truststore.jks"
      ssl_truststore_password => "${KAFKA_TRUSTSTORE_PASSWORD}"
      ssl_keystore_location => "/opt/logstash/config/certs/kafka.keystore.jks"
      ssl_keystore_password => "${KAFKA_KEYSTORE_PASSWORD}"
    }
  }
  
  # Dead letter queue for failed events
  if "_jsonparsefailure" in [tags] or "_grokparsefailure" in [tags] {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOSTS:localhost:9200}"]
      index => "isectech-failed-logs-%{+yyyy.MM.dd}"
      ssl => true
      ssl_certificate_verification => true
      cacert => "/opt/logstash/config/certs/ca.crt"
      user => "${ELASTICSEARCH_USERNAME}"
      password => "${ELASTICSEARCH_PASSWORD}"
    }
  }
  
  # Debug output for development
  if "${LOGSTASH_DEBUG:false}" == "true" {
    stdout {
      codec => rubydebug
    }
  }
}