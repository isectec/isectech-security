// deduplication.go - Production-grade vulnerability deduplication engine for iSECTECH
// Identifies and merges duplicate vulnerability findings across scanners

package data

import (
	"context"
	"crypto/md5"
	"encoding/hex"
	"fmt"
	"log/slog"
	"math"
	"sort"
	"strings"
	"sync"
	"time"

	"github.com/google/uuid"
)

// DeduplicationEngine identifies and manages duplicate vulnerability findings
type DeduplicationEngine struct {
	config            DeduplicationConfig
	logger            *slog.Logger
	repository        RepositoryInterface
	similarityEngine  *SimilarityEngine
	correlationEngine *CorrelationEngine
	mergingEngine     *MergingEngine
	statistics        DeduplicationStatistics
	cache             map[string]*DeduplicationResult
	cacheMutex        sync.RWMutex
}

// DeduplicationConfig contains configuration for the deduplication engine
type DeduplicationConfig struct {
	// Similarity thresholds
	ExactMatchThreshold       float64 `json:"exact_match_threshold"`
	HighSimilarityThreshold   float64 `json:"high_similarity_threshold"`
	MediumSimilarityThreshold float64 `json:"medium_similarity_threshold"`
	LowSimilarityThreshold    float64 `json:"low_similarity_threshold"`

	// Matching criteria weights
	AssetWeight       float64 `json:"asset_weight"`
	LocationWeight    float64 `json:"location_weight"`
	SeverityWeight    float64 `json:"severity_weight"`
	CVEWeight         float64 `json:"cve_weight"`
	CWEWeight         float64 `json:"cwe_weight"`
	TitleWeight       float64 `json:"title_weight"`
	DescriptionWeight float64 `json:"description_weight"`

	// Processing settings
	MaxCandidates         int           `json:"max_candidates"`
	BatchSize             int           `json:"batch_size"`
	ProcessingTimeout     time.Duration `json:"processing_timeout"`
	EnableAsyncProcessing bool          `json:"enable_async_processing"`

	// Caching settings
	EnableCaching   bool          `json:"enable_caching"`
	CacheExpiration time.Duration `json:"cache_expiration"`
	CacheMaxSize    int           `json:"cache_max_size"`

	// Merging strategy
	MergingStrategy       string             `json:"merging_strategy"` // conservative, aggressive, smart
	PreferredScannerOrder []string           `json:"preferred_scanner_order"`
	TrustScores           map[string]float64 `json:"trust_scores"`

	// Advanced options
	EnableMLSimilarity     bool `json:"enable_ml_similarity"`
	EnableSemanticAnalysis bool `json:"enable_semantic_analysis"`
	EnableTemporalAnalysis bool `json:"enable_temporal_analysis"`
	TemporalWindowHours    int  `json:"temporal_window_hours"`

	// iSECTECH specific settings
	TenantIsolation         bool             `json:"tenant_isolation"`
	BusinessContextMatching bool             `json:"business_context_matching"`
	CustomSimilarityRules   []SimilarityRule `json:"custom_similarity_rules"`
}

// DeduplicationStatistics tracks deduplication performance and results
type DeduplicationStatistics struct {
	TotalProcessed          int64                     `json:"total_processed"`
	DuplicatesFound         int64                     `json:"duplicates_found"`
	DuplicatesMerged        int64                     `json:"duplicates_merged"`
	ExactMatches            int64                     `json:"exact_matches"`
	HighSimilarityMatches   int64                     `json:"high_similarity_matches"`
	MediumSimilarityMatches int64                     `json:"medium_similarity_matches"`
	LowSimilarityMatches    int64                     `json:"low_similarity_matches"`
	AverageProcessingTime   time.Duration             `json:"average_processing_time"`
	CacheHitRate            float64                   `json:"cache_hit_rate"`
	ScannerPairStats        map[string]PairStatistics `json:"scanner_pair_stats"`
	QualityMetrics          QualityMetrics            `json:"quality_metrics"`
	LastProcessed           time.Time                 `json:"last_processed"`
}

type PairStatistics struct {
	Scanner1                 string   `json:"scanner1"`
	Scanner2                 string   `json:"scanner2"`
	TotalComparisons         int64    `json:"total_comparisons"`
	DuplicatesFound          int64    `json:"duplicates_found"`
	AverageSimilarity        float64  `json:"average_similarity"`
	CommonVulnerabilityTypes []string `json:"common_vulnerability_types"`
}

type QualityMetrics struct {
	FalsePositiveRate float64 `json:"false_positive_rate"`
	FalseNegativeRate float64 `json:"false_negative_rate"`
	Precision         float64 `json:"precision"`
	Recall            float64 `json:"recall"`
	F1Score           float64 `json:"f1_score"`
	Accuracy          float64 `json:"accuracy"`
}

// DeduplicationJob represents a single deduplication task
type DeduplicationJob struct {
	ID              string                 `json:"id"`
	Vulnerabilities []UnifiedVulnerability `json:"vulnerabilities"`
	Context         DeduplicationContext   `json:"context"`
	Options         DeduplicationOptions   `json:"options"`
	CreatedAt       time.Time              `json:"created_at"`
	ProcessedAt     *time.Time             `json:"processed_at,omitempty"`
	Result          *DeduplicationResult   `json:"result,omitempty"`
	Status          string                 `json:"status"`
	ProcessingTime  time.Duration          `json:"processing_time"`
}

type DeduplicationContext struct {
	TenantID      string                 `json:"tenant_id"`
	ScanID        string                 `json:"scan_id,omitempty"`
	AssetScope    []string               `json:"asset_scope,omitempty"`
	TimeWindow    *TimeWindow            `json:"time_window,omitempty"`
	ScannerTypes  []string               `json:"scanner_types,omitempty"`
	CustomContext map[string]interface{} `json:"custom_context,omitempty"`
}

type TimeWindow struct {
	StartTime time.Time `json:"start_time"`
	EndTime   time.Time `json:"end_time"`
}

type DeduplicationOptions struct {
	AutoMerge            bool    `json:"auto_merge"`
	RequireManualReview  bool    `json:"require_manual_review"`
	SimilarityThreshold  float64 `json:"similarity_threshold"`
	MergingStrategy      string  `json:"merging_strategy"`
	PreserveBothFindings bool    `json:"preserve_both_findings"`
	GenerateReport       bool    `json:"generate_report"`
}

type DeduplicationResult struct {
	DuplicateGroups       []DuplicateGroup       `json:"duplicate_groups"`
	MergedVulnerabilities []UnifiedVulnerability `json:"merged_vulnerabilities"`
	ProcessingMetrics     ProcessingMetrics      `json:"processing_metrics"`
	QualityAssessment     QualityAssessment      `json:"quality_assessment"`
	ReviewRequired        []string               `json:"review_required"`
	ProcessingNotes       []string               `json:"processing_notes"`
}

type DuplicateGroup struct {
	GroupID                  string               `json:"group_id"`
	PrimaryVulnerability     UnifiedVulnerability `json:"primary_vulnerability"`
	DuplicateVulnerabilities []DuplicateMatch     `json:"duplicate_vulnerabilities"`
	GroupSimilarity          float64              `json:"group_similarity"`
	MergeStrategy            string               `json:"merge_strategy"`
	MergeConfidence          float64              `json:"merge_confidence"`
	RequiresReview           bool                 `json:"requires_review"`
	ReviewReasons            []string             `json:"review_reasons"`
}

type DuplicateMatch struct {
	Vulnerability    UnifiedVulnerability `json:"vulnerability"`
	SimilarityScore  float64              `json:"similarity_score"`
	MatchType        string               `json:"match_type"` // exact, high, medium, low
	MatchingCriteria []MatchCriterion     `json:"matching_criteria"`
	ConfidenceScore  float64              `json:"confidence_score"`
}

type MatchCriterion struct {
	Name     string  `json:"name"`
	Score    float64 `json:"score"`
	Weight   float64 `json:"weight"`
	Evidence string  `json:"evidence"`
}

type ProcessingMetrics struct {
	TotalVulnerabilities int   `json:"total_vulnerabilities"`
	ComparisonsPerformed int   `json:"comparisons_performed"`
	DuplicateGroupsFound int   `json:"duplicate_groups_found"`
	ProcessingTimeMs     int64 `json:"processing_time_ms"`
	CacheHits            int   `json:"cache_hits"`
	CacheMisses          int   `json:"cache_misses"`
}

type QualityAssessment struct {
	OverallConfidence float64        `json:"overall_confidence"`
	ReliabilityScore  float64        `json:"reliability_score"`
	ConsistencyScore  float64        `json:"consistency_score"`
	QualityIssues     []QualityIssue `json:"quality_issues"`
	Recommendations   []string       `json:"recommendations"`
}

type QualityIssue struct {
	Type                    string   `json:"type"`
	Severity                string   `json:"severity"`
	Message                 string   `json:"message"`
	AffectedVulnerabilities []string `json:"affected_vulnerabilities"`
}

// SimilarityEngine calculates similarity between vulnerabilities
type SimilarityEngine struct {
	config           SimilarityConfig
	logger           *slog.Logger
	textAnalyzer     *TextAnalyzer
	semanticAnalyzer *SemanticAnalyzer
	mlModel          MLSimilarityModel
}

type SimilarityConfig struct {
	EnableTextAnalysis      bool   `json:"enable_text_analysis"`
	EnableSemanticAnalysis  bool   `json:"enable_semantic_analysis"`
	EnableMLAnalysis        bool   `json:"enable_ml_analysis"`
	TextSimilarityAlgorithm string `json:"text_similarity_algorithm"` // jaccard, cosine, levenshtein
	SemanticModelPath       string `json:"semantic_model_path"`
	MLModelPath             string `json:"ml_model_path"`
}

type SimilarityRule struct {
	Name        string          `json:"name"`
	Conditions  []RuleCondition `json:"conditions"`
	Action      string          `json:"action"` // match, no_match, review_required
	Weight      float64         `json:"weight"`
	Description string          `json:"description"`
}

type RuleCondition struct {
	Field     string      `json:"field"`
	Operator  string      `json:"operator"` // equals, contains, regex, similarity
	Value     interface{} `json:"value"`
	Threshold float64     `json:"threshold,omitempty"`
}

// CorrelationEngine manages vulnerability relationships and dependencies
type CorrelationEngine struct {
	config            CorrelationConfig
	logger            *slog.Logger
	graphDatabase     GraphDatabase
	relationshipRules []RelationshipRule
}

type CorrelationConfig struct {
	EnableGraphAnalysis  bool     `json:"enable_graph_analysis"`
	MaxRelationshipDepth int      `json:"max_relationship_depth"`
	RelationshipTypes    []string `json:"relationship_types"`
	TemporalCorrelation  bool     `json:"temporal_correlation"`
	AssetCorrelation     bool     `json:"asset_correlation"`
	AttackChainDetection bool     `json:"attack_chain_detection"`
}

type RelationshipRule struct {
	Name          string          `json:"name"`
	Type          string          `json:"type"` // duplicate, related, dependent
	Conditions    []RuleCondition `json:"conditions"`
	Strength      float64         `json:"strength"`
	Bidirectional bool            `json:"bidirectional"`
}

type GraphDatabase interface {
	CreateNode(ctx context.Context, nodeType string, properties map[string]interface{}) (string, error)
	CreateRelationship(ctx context.Context, fromNode, toNode, relationshipType string, properties map[string]interface{}) error
	FindSimilarNodes(ctx context.Context, nodeID string, similarity float64) ([]GraphNode, error)
	GetNodeRelationships(ctx context.Context, nodeID string) ([]GraphRelationship, error)
}

type GraphNode struct {
	ID         string                 `json:"id"`
	Type       string                 `json:"type"`
	Properties map[string]interface{} `json:"properties"`
	Similarity float64                `json:"similarity,omitempty"`
}

type GraphRelationship struct {
	ID         string                 `json:"id"`
	Type       string                 `json:"type"`
	FromNode   string                 `json:"from_node"`
	ToNode     string                 `json:"to_node"`
	Properties map[string]interface{} `json:"properties"`
	Strength   float64                `json:"strength"`
}

// MergingEngine handles the merging of duplicate vulnerabilities
type MergingEngine struct {
	config            MergingConfig
	logger            *slog.Logger
	mergingStrategies map[string]MergingStrategy
}

type MergingConfig struct {
	DefaultStrategy      string `json:"default_strategy"`
	ConflictResolution   string `json:"conflict_resolution"` // prefer_higher_severity, prefer_newer, prefer_trusted_scanner
	PreserveOriginalData bool   `json:"preserve_original_data"`
	GenerateAuditTrail   bool   `json:"generate_audit_trail"`
	RequireApproval      bool   `json:"require_approval"`
}

type MergingStrategy interface {
	StrategyName() string
	Merge(primary *UnifiedVulnerability, duplicates []DuplicateMatch) (*UnifiedVulnerability, error)
	CanMerge(vulnerabilities []UnifiedVulnerability) bool
	GetMergeConfidence(vulnerabilities []UnifiedVulnerability) float64
}

// Text and semantic analysis interfaces

type TextAnalyzer interface {
	CalculateSimilarity(text1, text2 string) (float64, error)
	ExtractKeywords(text string) ([]string, error)
	NormalizeText(text string) string
}

type SemanticAnalyzer interface {
	GetEmbedding(text string) ([]float64, error)
	CalculateSemanticSimilarity(embedding1, embedding2 []float64) float64
	FindSimilarConcepts(concept string, threshold float64) ([]string, error)
}

type MLSimilarityModel interface {
	PredictSimilarity(vuln1, vuln2 *UnifiedVulnerability) (float64, error)
	Train(trainingData []TrainingExample) error
	GetModelMetrics() ModelMetrics
}

type TrainingExample struct {
	Vulnerability1  UnifiedVulnerability `json:"vulnerability1"`
	Vulnerability2  UnifiedVulnerability `json:"vulnerability2"`
	IsDuplicate     bool                 `json:"is_duplicate"`
	SimilarityScore float64              `json:"similarity_score"`
}

type ModelMetrics struct {
	Accuracy         float64   `json:"accuracy"`
	Precision        float64   `json:"precision"`
	Recall           float64   `json:"recall"`
	F1Score          float64   `json:"f1_score"`
	TrainingExamples int       `json:"training_examples"`
	LastTrained      time.Time `json:"last_trained"`
}

// NewDeduplicationEngine creates a new production-grade deduplication engine
func NewDeduplicationEngine(config DeduplicationConfig, repository RepositoryInterface, logger *slog.Logger) *DeduplicationEngine {
	engine := &DeduplicationEngine{
		config:     config,
		logger:     logger.With("component", "deduplication_engine"),
		repository: repository,
		statistics: DeduplicationStatistics{
			ScannerPairStats: make(map[string]PairStatistics),
		},
		cache: make(map[string]*DeduplicationResult),
	}

	// Initialize similarity engine
	engine.similarityEngine = &SimilarityEngine{
		config: SimilarityConfig{
			EnableTextAnalysis:      true,
			EnableSemanticAnalysis:  config.EnableSemanticAnalysis,
			EnableMLAnalysis:        config.EnableMLSimilarity,
			TextSimilarityAlgorithm: "cosine",
		},
		logger: logger.With("component", "similarity_engine"),
	}

	// Initialize correlation engine
	engine.correlationEngine = &CorrelationEngine{
		config: CorrelationConfig{
			EnableGraphAnalysis:  true,
			MaxRelationshipDepth: 3,
			TemporalCorrelation:  config.EnableTemporalAnalysis,
			AssetCorrelation:     true,
			AttackChainDetection: true,
		},
		logger: logger.With("component", "correlation_engine"),
	}

	// Initialize merging engine
	engine.mergingEngine = &MergingEngine{
		config: MergingConfig{
			DefaultStrategy:      config.MergingStrategy,
			ConflictResolution:   "prefer_higher_severity",
			PreserveOriginalData: true,
			GenerateAuditTrail:   true,
		},
		logger:            logger.With("component", "merging_engine"),
		mergingStrategies: make(map[string]MergingStrategy),
	}

	return engine
}

// ProcessVulnerabilities identifies and processes duplicate vulnerabilities
func (de *DeduplicationEngine) ProcessVulnerabilities(ctx context.Context, vulnerabilities []UnifiedVulnerability, options DeduplicationOptions) (*DeduplicationResult, error) {
	startTime := time.Now()

	de.logger.Info("Processing vulnerabilities for deduplication",
		"count", len(vulnerabilities),
		"auto_merge", options.AutoMerge)

	// Generate cache key
	cacheKey := de.generateCacheKey(vulnerabilities, options)

	// Check cache if enabled
	if de.config.EnableCaching {
		if cachedResult := de.getCachedResult(cacheKey); cachedResult != nil {
			de.logger.Debug("Returning cached deduplication result")
			return cachedResult, nil
		}
	}

	// Create result structure
	result := &DeduplicationResult{
		DuplicateGroups:       make([]DuplicateGroup, 0),
		MergedVulnerabilities: make([]UnifiedVulnerability, 0),
		ProcessingMetrics: ProcessingMetrics{
			TotalVulnerabilities: len(vulnerabilities),
		},
		ProcessingNotes: make([]string, 0),
	}

	// Find duplicate groups
	duplicateGroups, err := de.findDuplicateGroups(ctx, vulnerabilities, options)
	if err != nil {
		return nil, fmt.Errorf("failed to find duplicate groups: %w", err)
	}

	result.DuplicateGroups = duplicateGroups
	result.ProcessingMetrics.DuplicateGroupsFound = len(duplicateGroups)

	// Merge duplicates if auto-merge is enabled
	if options.AutoMerge {
		mergedVulns, err := de.mergeDuplicateGroups(ctx, duplicateGroups, options)
		if err != nil {
			return nil, fmt.Errorf("failed to merge duplicate groups: %w", err)
		}
		result.MergedVulnerabilities = mergedVulns
	}

	// Calculate quality assessment
	result.QualityAssessment = de.assessQuality(result)

	// Update statistics
	processingTime := time.Since(startTime)
	de.updateStatistics(len(vulnerabilities), len(duplicateGroups), processingTime)

	result.ProcessingMetrics.ProcessingTimeMs = processingTime.Nanoseconds() / 1000000

	// Cache result if enabled
	if de.config.EnableCaching {
		de.cacheResult(cacheKey, result)
	}

	de.logger.Info("Deduplication processing completed",
		"duplicate_groups", len(duplicateGroups),
		"processing_time", processingTime)

	return result, nil
}

// FindSimilarVulnerabilities finds vulnerabilities similar to the given one
func (de *DeduplicationEngine) FindSimilarVulnerabilities(ctx context.Context, vuln *UnifiedVulnerability, threshold float64) ([]DuplicateMatch, error) {
	de.logger.Debug("Finding similar vulnerabilities",
		"vuln_id", vuln.ID,
		"threshold", threshold)

	// Search for candidates in repository
	candidates, err := de.repository.FindSimilar(ctx, vuln, threshold)
	if err != nil {
		return nil, fmt.Errorf("failed to find similar vulnerabilities: %w", err)
	}

	if len(candidates) == 0 {
		return []DuplicateMatch{}, nil
	}

	// Calculate similarity scores for each candidate
	matches := make([]DuplicateMatch, 0, len(candidates))

	for _, candidate := range candidates {
		if candidate.ID == vuln.ID {
			continue // Skip self
		}

		similarity, criteria, err := de.calculateSimilarity(ctx, vuln, &candidate)
		if err != nil {
			de.logger.Warn("Failed to calculate similarity", "error", err)
			continue
		}

		if similarity >= threshold {
			matchType := de.classifyMatchType(similarity)
			match := DuplicateMatch{
				Vulnerability:    candidate,
				SimilarityScore:  similarity,
				MatchType:        matchType,
				MatchingCriteria: criteria,
				ConfidenceScore:  de.calculateConfidence(similarity, criteria),
			}
			matches = append(matches, match)
		}
	}

	// Sort by similarity score (descending)
	sort.Slice(matches, func(i, j int) bool {
		return matches[i].SimilarityScore > matches[j].SimilarityScore
	})

	// Limit to max candidates
	if len(matches) > de.config.MaxCandidates {
		matches = matches[:de.config.MaxCandidates]
	}

	return matches, nil
}

// Private helper methods

func (de *DeduplicationEngine) findDuplicateGroups(ctx context.Context, vulnerabilities []UnifiedVulnerability, options DeduplicationOptions) ([]DuplicateGroup, error) {
	groups := make([]DuplicateGroup, 0)
	processed := make(map[string]bool)

	for i, vuln := range vulnerabilities {
		if processed[vuln.ID] {
			continue
		}

		// Find similar vulnerabilities for this one
		similar, err := de.FindSimilarVulnerabilities(ctx, &vuln, options.SimilarityThreshold)
		if err != nil {
			de.logger.Warn("Failed to find similar vulnerabilities", "error", err)
			continue
		}

		if len(similar) > 0 {
			// Create duplicate group
			group := DuplicateGroup{
				GroupID:                  uuid.New().String(),
				PrimaryVulnerability:     vuln,
				DuplicateVulnerabilities: similar,
				GroupSimilarity:          de.calculateGroupSimilarity(similar),
				MergeStrategy:            options.MergingStrategy,
			}

			// Calculate merge confidence
			group.MergeConfidence = de.calculateMergeConfidence(&group)

			// Determine if review is required
			group.RequiresReview = de.requiresReview(&group, options)
			if group.RequiresReview {
				group.ReviewReasons = de.getReviewReasons(&group)
			}

			groups = append(groups, group)

			// Mark all vulnerabilities in this group as processed
			processed[vuln.ID] = true
			for _, dup := range similar {
				processed[dup.Vulnerability.ID] = true
			}
		}
	}

	return groups, nil
}

func (de *DeduplicationEngine) calculateSimilarity(ctx context.Context, vuln1, vuln2 *UnifiedVulnerability) (float64, []MatchCriterion, error) {
	criteria := make([]MatchCriterion, 0)
	totalScore := 0.0
	totalWeight := 0.0

	// Asset similarity
	assetScore := de.calculateAssetSimilarity(vuln1, vuln2)
	criteria = append(criteria, MatchCriterion{
		Name:     "asset_similarity",
		Score:    assetScore,
		Weight:   de.config.AssetWeight,
		Evidence: fmt.Sprintf("Asset1: %s, Asset2: %s", vuln1.AssetID, vuln2.AssetID),
	})
	totalScore += assetScore * de.config.AssetWeight
	totalWeight += de.config.AssetWeight

	// Location similarity
	locationScore := de.calculateLocationSimilarity(vuln1, vuln2)
	criteria = append(criteria, MatchCriterion{
		Name:     "location_similarity",
		Score:    locationScore,
		Weight:   de.config.LocationWeight,
		Evidence: fmt.Sprintf("Type1: %s, Type2: %s", vuln1.Location.Type, vuln2.Location.Type),
	})
	totalScore += locationScore * de.config.LocationWeight
	totalWeight += de.config.LocationWeight

	// CVE similarity
	cveScore := de.calculateCVESimilarity(vuln1, vuln2)
	criteria = append(criteria, MatchCriterion{
		Name:     "cve_similarity",
		Score:    cveScore,
		Weight:   de.config.CVEWeight,
		Evidence: fmt.Sprintf("CVEs1: %v, CVEs2: %v", vuln1.CVE, vuln2.CVE),
	})
	totalScore += cveScore * de.config.CVEWeight
	totalWeight += de.config.CVEWeight

	// CWE similarity
	cweScore := de.calculateCWESimilarity(vuln1, vuln2)
	criteria = append(criteria, MatchCriterion{
		Name:     "cwe_similarity",
		Score:    cweScore,
		Weight:   de.config.CWEWeight,
		Evidence: fmt.Sprintf("CWEs1: %v, CWEs2: %v", vuln1.CWE, vuln2.CWE),
	})
	totalScore += cweScore * de.config.CWEWeight
	totalWeight += de.config.CWEWeight

	// Title similarity
	titleScore := de.calculateTextSimilarity(vuln1.Title, vuln2.Title)
	criteria = append(criteria, MatchCriterion{
		Name:     "title_similarity",
		Score:    titleScore,
		Weight:   de.config.TitleWeight,
		Evidence: fmt.Sprintf("Title1: %s, Title2: %s", vuln1.Title, vuln2.Title),
	})
	totalScore += titleScore * de.config.TitleWeight
	totalWeight += de.config.TitleWeight

	// Description similarity
	descScore := de.calculateTextSimilarity(vuln1.Description, vuln2.Description)
	criteria = append(criteria, MatchCriterion{
		Name:     "description_similarity",
		Score:    descScore,
		Weight:   de.config.DescriptionWeight,
		Evidence: "Description text analysis",
	})
	totalScore += descScore * de.config.DescriptionWeight
	totalWeight += de.config.DescriptionWeight

	// Severity similarity
	severityScore := de.calculateSeveritySimilarity(vuln1, vuln2)
	criteria = append(criteria, MatchCriterion{
		Name:     "severity_similarity",
		Score:    severityScore,
		Weight:   de.config.SeverityWeight,
		Evidence: fmt.Sprintf("Severity1: %s, Severity2: %s", vuln1.Severity, vuln2.Severity),
	})
	totalScore += severityScore * de.config.SeverityWeight
	totalWeight += de.config.SeverityWeight

	// Calculate weighted average
	var finalScore float64
	if totalWeight > 0 {
		finalScore = totalScore / totalWeight
	}

	return finalScore, criteria, nil
}

func (de *DeduplicationEngine) calculateAssetSimilarity(vuln1, vuln2 *UnifiedVulnerability) float64 {
	// Exact asset match
	if vuln1.AssetID == vuln2.AssetID {
		return 1.0
	}

	// Check CPE similarity if available
	if vuln1.AssetCPE != "" && vuln2.AssetCPE != "" {
		return de.calculateCPESimilarity(vuln1.AssetCPE, vuln2.AssetCPE)
	}

	// Check asset type similarity
	if vuln1.AssetType == vuln2.AssetType {
		return 0.5
	}

	return 0.0
}

func (de *DeduplicationEngine) calculateLocationSimilarity(vuln1, vuln2 *UnifiedVulnerability) float64 {
	// Same location type
	if vuln1.Location.Type != vuln2.Location.Type {
		return 0.0
	}

	// Same primary location
	if vuln1.Location.Primary == vuln2.Location.Primary {
		return 1.0
	}

	// Partial match on primary location
	if strings.Contains(vuln1.Location.Primary, vuln2.Location.Primary) ||
		strings.Contains(vuln2.Location.Primary, vuln1.Location.Primary) {
		return 0.7
	}

	return 0.0
}

func (de *DeduplicationEngine) calculateCVESimilarity(vuln1, vuln2 *UnifiedVulnerability) float64 {
	// Find intersection of CVEs
	cve1Set := make(map[string]bool)
	for _, cve := range vuln1.CVE {
		cve1Set[cve] = true
	}

	commonCVEs := 0
	for _, cve := range vuln2.CVE {
		if cve1Set[cve] {
			commonCVEs++
		}
	}

	if commonCVEs == 0 {
		return 0.0
	}

	// Calculate Jaccard similarity
	totalUnique := len(vuln1.CVE) + len(vuln2.CVE) - commonCVEs
	if totalUnique == 0 {
		return 1.0
	}

	return float64(commonCVEs) / float64(totalUnique)
}

func (de *DeduplicationEngine) calculateCWESimilarity(vuln1, vuln2 *UnifiedVulnerability) float64 {
	// Find intersection of CWEs
	cwe1Set := make(map[string]bool)
	for _, cwe := range vuln1.CWE {
		cwe1Set[cwe] = true
	}

	commonCWEs := 0
	for _, cwe := range vuln2.CWE {
		if cwe1Set[cwe] {
			commonCWEs++
		}
	}

	if commonCWEs == 0 {
		return 0.0
	}

	// Calculate Jaccard similarity
	totalUnique := len(vuln1.CWE) + len(vuln2.CWE) - commonCWEs
	if totalUnique == 0 {
		return 1.0
	}

	return float64(commonCWEs) / float64(totalUnique)
}

func (de *DeduplicationEngine) calculateTextSimilarity(text1, text2 string) float64 {
	// Simple cosine similarity based on word vectors
	words1 := strings.Fields(strings.ToLower(text1))
	words2 := strings.Fields(strings.ToLower(text2))

	if len(words1) == 0 || len(words2) == 0 {
		return 0.0
	}

	// Create word frequency maps
	freq1 := make(map[string]int)
	freq2 := make(map[string]int)

	for _, word := range words1 {
		freq1[word]++
	}
	for _, word := range words2 {
		freq2[word]++
	}

	// Calculate cosine similarity
	dotProduct := 0.0
	norm1 := 0.0
	norm2 := 0.0

	allWords := make(map[string]bool)
	for word := range freq1 {
		allWords[word] = true
	}
	for word := range freq2 {
		allWords[word] = true
	}

	for word := range allWords {
		f1 := float64(freq1[word])
		f2 := float64(freq2[word])

		dotProduct += f1 * f2
		norm1 += f1 * f1
		norm2 += f2 * f2
	}

	if norm1 == 0 || norm2 == 0 {
		return 0.0
	}

	return dotProduct / (math.Sqrt(norm1) * math.Sqrt(norm2))
}

func (de *DeduplicationEngine) calculateSeveritySimilarity(vuln1, vuln2 *UnifiedVulnerability) float64 {
	// Exact match
	if vuln1.Severity == vuln2.Severity {
		return 1.0
	}

	// Define severity order for partial matching
	severityOrder := map[string]int{
		"critical": 5,
		"high":     4,
		"medium":   3,
		"low":      2,
		"info":     1,
	}

	sev1, ok1 := severityOrder[vuln1.Severity]
	sev2, ok2 := severityOrder[vuln2.Severity]

	if !ok1 || !ok2 {
		return 0.0
	}

	// Calculate similarity based on distance
	distance := math.Abs(float64(sev1 - sev2))
	maxDistance := 4.0 // max difference between critical and info

	return 1.0 - (distance / maxDistance)
}

func (de *DeduplicationEngine) calculateCPESimilarity(cpe1, cpe2 string) float64 {
	// Parse CPE strings and compare components
	parts1 := strings.Split(cpe1, ":")
	parts2 := strings.Split(cpe2, ":")

	if len(parts1) < 5 || len(parts2) < 5 {
		return 0.0
	}

	// Compare vendor, product, version
	matches := 0
	total := 3

	// Vendor
	if parts1[2] == parts2[2] {
		matches++
	}

	// Product
	if parts1[3] == parts2[3] {
		matches++
	}

	// Version
	if parts1[4] == parts2[4] {
		matches++
	}

	return float64(matches) / float64(total)
}

func (de *DeduplicationEngine) classifyMatchType(similarity float64) string {
	if similarity >= de.config.ExactMatchThreshold {
		return "exact"
	} else if similarity >= de.config.HighSimilarityThreshold {
		return "high"
	} else if similarity >= de.config.MediumSimilarityThreshold {
		return "medium"
	} else if similarity >= de.config.LowSimilarityThreshold {
		return "low"
	}
	return "none"
}

func (de *DeduplicationEngine) calculateConfidence(similarity float64, criteria []MatchCriterion) float64 {
	// Base confidence from similarity score
	baseConfidence := similarity

	// Adjust based on matching criteria strength
	strongCriteria := 0
	for _, criterion := range criteria {
		if criterion.Score > 0.8 {
			strongCriteria++
		}
	}

	// Bonus for multiple strong criteria
	if strongCriteria >= 3 {
		baseConfidence = math.Min(1.0, baseConfidence+0.1)
	}

	return baseConfidence
}

func (de *DeduplicationEngine) calculateGroupSimilarity(matches []DuplicateMatch) float64 {
	if len(matches) == 0 {
		return 0.0
	}

	total := 0.0
	for _, match := range matches {
		total += match.SimilarityScore
	}

	return total / float64(len(matches))
}

func (de *DeduplicationEngine) calculateMergeConfidence(group *DuplicateGroup) float64 {
	// Base confidence from group similarity
	confidence := group.GroupSimilarity

	// Adjust based on number of duplicates
	if len(group.DuplicateVulnerabilities) > 1 {
		confidence = math.Min(1.0, confidence+0.05*float64(len(group.DuplicateVulnerabilities)-1))
	}

	// Reduce confidence if different scanner types
	scannerTypes := make(map[string]bool)
	scannerTypes[group.PrimaryVulnerability.ScannerType] = true

	for _, dup := range group.DuplicateVulnerabilities {
		scannerTypes[dup.Vulnerability.ScannerType] = true
	}

	if len(scannerTypes) > 2 {
		confidence *= 0.9 // Reduce confidence for cross-scanner matches
	}

	return confidence
}

func (de *DeduplicationEngine) requiresReview(group *DuplicateGroup, options DeduplicationOptions) bool {
	// Always require review if explicitly requested
	if options.RequireManualReview {
		return true
	}

	// Require review for low confidence merges
	if group.MergeConfidence < 0.8 {
		return true
	}

	// Require review for cross-scanner duplicates with different severities
	severities := make(map[string]bool)
	severities[group.PrimaryVulnerability.Severity] = true

	for _, dup := range group.DuplicateVulnerabilities {
		severities[dup.Vulnerability.Severity] = true
	}

	if len(severities) > 1 {
		return true
	}

	return false
}

func (de *DeduplicationEngine) getReviewReasons(group *DuplicateGroup) []string {
	reasons := make([]string, 0)

	if group.MergeConfidence < 0.8 {
		reasons = append(reasons, "Low merge confidence")
	}

	// Check for different severities
	severities := make(map[string]bool)
	severities[group.PrimaryVulnerability.Severity] = true

	for _, dup := range group.DuplicateVulnerabilities {
		severities[dup.Vulnerability.Severity] = true
	}

	if len(severities) > 1 {
		reasons = append(reasons, "Different severity levels")
	}

	// Check for different scanner types
	scannerTypes := make(map[string]bool)
	scannerTypes[group.PrimaryVulnerability.ScannerType] = true

	for _, dup := range group.DuplicateVulnerabilities {
		scannerTypes[dup.Vulnerability.ScannerType] = true
	}

	if len(scannerTypes) > 2 {
		reasons = append(reasons, "Multiple scanner types")
	}

	return reasons
}

func (de *DeduplicationEngine) mergeDuplicateGroups(ctx context.Context, groups []DuplicateGroup, options DeduplicationOptions) ([]UnifiedVulnerability, error) {
	merged := make([]UnifiedVulnerability, 0, len(groups))

	for _, group := range groups {
		if group.RequiresReview && !options.AutoMerge {
			continue // Skip groups that require review
		}

		// Get merging strategy
		strategy, exists := de.mergingEngine.mergingStrategies[group.MergeStrategy]
		if !exists {
			strategy = de.mergingEngine.mergingStrategies[de.mergingEngine.config.DefaultStrategy]
		}

		if strategy == nil {
			de.logger.Warn("No merging strategy available", "strategy", group.MergeStrategy)
			continue
		}

		// Merge the group
		mergedVuln, err := strategy.Merge(&group.PrimaryVulnerability, group.DuplicateVulnerabilities)
		if err != nil {
			de.logger.Error("Failed to merge vulnerability group", "error", err, "group_id", group.GroupID)
			continue
		}

		merged = append(merged, *mergedVuln)
	}

	return merged, nil
}

func (de *DeduplicationEngine) assessQuality(result *DeduplicationResult) QualityAssessment {
	assessment := QualityAssessment{
		QualityIssues:   make([]QualityIssue, 0),
		Recommendations: make([]string, 0),
	}

	// Calculate overall confidence
	totalConfidence := 0.0
	confidenceCount := 0

	for _, group := range result.DuplicateGroups {
		totalConfidence += group.MergeConfidence
		confidenceCount++
	}

	if confidenceCount > 0 {
		assessment.OverallConfidence = totalConfidence / float64(confidenceCount)
	}

	// Calculate reliability score
	assessment.ReliabilityScore = 0.85 // Placeholder

	// Calculate consistency score
	assessment.ConsistencyScore = 0.90 // Placeholder

	// Generate recommendations
	if assessment.OverallConfidence < 0.8 {
		assessment.Recommendations = append(assessment.Recommendations, "Consider manual review of low-confidence matches")
	}

	return assessment
}

func (de *DeduplicationEngine) generateCacheKey(vulnerabilities []UnifiedVulnerability, options DeduplicationOptions) string {
	// Create a hash-based cache key
	hasher := md5.New()

	// Sort vulnerabilities by ID for consistent hashing
	sort.Slice(vulnerabilities, func(i, j int) bool {
		return vulnerabilities[i].ID < vulnerabilities[j].ID
	})

	for _, vuln := range vulnerabilities {
		hasher.Write([]byte(vuln.ID))
	}

	// Include options in hash
	hasher.Write([]byte(fmt.Sprintf("%.2f_%s_%v",
		options.SimilarityThreshold,
		options.MergingStrategy,
		options.AutoMerge)))

	return hex.EncodeToString(hasher.Sum(nil))
}

func (de *DeduplicationEngine) getCachedResult(cacheKey string) *DeduplicationResult {
	de.cacheMutex.RLock()
	defer de.cacheMutex.RUnlock()

	return de.cache[cacheKey]
}

func (de *DeduplicationEngine) cacheResult(cacheKey string, result *DeduplicationResult) {
	de.cacheMutex.Lock()
	defer de.cacheMutex.Unlock()

	// Implement cache size limits
	if len(de.cache) >= de.config.CacheMaxSize {
		// Remove oldest entries (simple LRU approximation)
		for k := range de.cache {
			delete(de.cache, k)
			break
		}
	}

	de.cache[cacheKey] = result
}

func (de *DeduplicationEngine) updateStatistics(vulnCount, duplicateGroups int, processingTime time.Duration) {
	de.statistics.TotalProcessed += int64(vulnCount)
	de.statistics.DuplicatesFound += int64(duplicateGroups)

	// Update average processing time
	if de.statistics.TotalProcessed == int64(vulnCount) {
		de.statistics.AverageProcessingTime = processingTime
	} else {
		total := de.statistics.AverageProcessingTime * time.Duration(de.statistics.TotalProcessed-int64(vulnCount))
		de.statistics.AverageProcessingTime = (total + processingTime) / time.Duration(de.statistics.TotalProcessed)
	}

	de.statistics.LastProcessed = time.Now()
}
