// iSECTECH Vulnerability Scanner - Scan Orchestration Service
// Production-grade scan management and multi-scanner coordination
// Copyright (c) 2024 iSECTECH. All rights reserved.

package service

import (
	"context"
	"fmt"
	"sort"
	"sync"
	"time"

	"github.com/google/uuid"
	"github.com/sirupsen/logrus"

	"github.com/isectech/backend/services/vulnerability-scanner/domain/entity"
)

// ScanOrchestrator manages and coordinates vulnerability scans across multiple scanners
type ScanOrchestrator struct {
	logger              *logrus.Logger
	scanners            map[string]VulnerabilityScanner
	queue               *ScanQueue
	scheduler           *ScanScheduler
	executor            *ScanExecutor
	resultProcessor     *ResultProcessor
	assetService        AssetServiceInterface
	notificationService NotificationServiceInterface
	mutex               sync.RWMutex
	config              OrchestratorConfig
	metrics             *OrchestratorMetrics
}

// OrchestratorConfig holds orchestrator configuration
type OrchestratorConfig struct {
	MaxConcurrentScans      int           `json:"max_concurrent_scans"`
	MaxQueueSize            int           `json:"max_queue_size"`
	ScanTimeout             time.Duration `json:"scan_timeout"`
	ResultRetentionPeriod   time.Duration `json:"result_retention_period"`
	EnableAutoScheduling    bool          `json:"enable_auto_scheduling"`
	EnableResultDedup       bool          `json:"enable_result_dedup"`
	EnableThreatIntel       bool          `json:"enable_threat_intel"`
	HealthCheckInterval     time.Duration `json:"health_check_interval"`
	MetricsRetention        time.Duration `json:"metrics_retention"`
	NotificationChannels    []string      `json:"notification_channels"`
	QueueProcessingInterval time.Duration `json:"queue_processing_interval"`
}

// OrchestratorMetrics tracks orchestrator performance
type OrchestratorMetrics struct {
	TotalScansRequested  int64               `json:"total_scans_requested"`
	TotalScansCompleted  int64               `json:"total_scans_completed"`
	TotalScansFailed     int64               `json:"total_scans_failed"`
	TotalScansCancelled  int64               `json:"total_scans_cancelled"`
	CurrentActiveScans   int                 `json:"current_active_scans"`
	CurrentQueueSize     int                 `json:"current_queue_size"`
	AverageScanDuration  time.Duration       `json:"average_scan_duration"`
	AverageQueueWaitTime time.Duration       `json:"average_queue_wait_time"`
	ScannerUtilization   map[string]float64  `json:"scanner_utilization"`
	LastMetricsUpdate    time.Time           `json:"last_metrics_update"`
	ThroughputPerHour    float64             `json:"throughput_per_hour"`
	SuccessRate          float64             `json:"success_rate"`
	ResourceUtilization  ResourceUtilization `json:"resource_utilization"`
}

// ResourceUtilization tracks resource usage
type ResourceUtilization struct {
	CPUUsage    float64 `json:"cpu_usage"`
	MemoryUsage float64 `json:"memory_usage"`
	DiskUsage   float64 `json:"disk_usage"`
	NetworkIO   int64   `json:"network_io"`
}

// ScanQueue manages scan job queuing and prioritization
type ScanQueue struct {
	items   []*QueueItem
	mutex   sync.RWMutex
	maxSize int
	metrics *QueueMetrics
}

// QueueItem represents a queued scan job
type QueueItem struct {
	ID           uuid.UUID         `json:"id"`
	Scan         *entity.Scan      `json:"scan"`
	Priority     int               `json:"priority"`
	QueuedAt     time.Time         `json:"queued_at"`
	ScheduledAt  time.Time         `json:"scheduled_at"`
	RetryCount   int               `json:"retry_count"`
	MaxRetries   int               `json:"max_retries"`
	Dependencies []uuid.UUID       `json:"dependencies"`
	Tags         map[string]string `json:"tags"`
}

// QueueMetrics tracks queue performance
type QueueMetrics struct {
	ItemsQueued     int64         `json:"items_queued"`
	ItemsProcessed  int64         `json:"items_processed"`
	ItemsDropped    int64         `json:"items_dropped"`
	AverageWaitTime time.Duration `json:"average_wait_time"`
	MaxWaitTime     time.Duration `json:"max_wait_time"`
	CurrentSize     int           `json:"current_size"`
	MaxSizeReached  int           `json:"max_size_reached"`
}

// ScanScheduler handles scan scheduling and automation
type ScanScheduler struct {
	schedules map[uuid.UUID]*ScheduleEntry
	mutex     sync.RWMutex
	ticker    *time.Ticker
	stopChan  chan bool
	isRunning bool
	logger    *logrus.Logger
}

// ScheduleEntry represents a scheduled scan
type ScheduleEntry struct {
	ID        uuid.UUID            `json:"id"`
	Scan      *entity.Scan         `json:"scan"`
	Schedule  *entity.ScanSchedule `json:"schedule"`
	NextRun   time.Time            `json:"next_run"`
	LastRun   *time.Time           `json:"last_run,omitempty"`
	IsActive  bool                 `json:"is_active"`
	CreatedAt time.Time            `json:"created_at"`
	UpdatedAt time.Time            `json:"updated_at"`
}

// ScanExecutor manages scan execution across multiple scanners
type ScanExecutor struct {
	activeSessions map[uuid.UUID]*ExecutionSession
	mutex          sync.RWMutex
	maxConcurrent  int
	logger         *logrus.Logger
}

// ExecutionSession tracks an active scan execution
type ExecutionSession struct {
	ID            uuid.UUID              `json:"id"`
	ScanID        uuid.UUID              `json:"scan_id"`
	Scanner       VulnerabilityScanner   `json:"-"`
	ScannerName   string                 `json:"scanner_name"`
	Status        entity.ScanStatus      `json:"status"`
	StartedAt     time.Time              `json:"started_at"`
	Progress      *ScanProgress          `json:"progress"`
	Context       context.Context        `json:"-"`
	CancelFunc    context.CancelFunc     `json:"-"`
	ResultChannel chan *ScanResults      `json:"-"`
	ErrorChannel  chan error             `json:"-"`
	Metadata      map[string]interface{} `json:"metadata"`
}

// ResultProcessor handles scan result processing and normalization
type ResultProcessor struct {
	processors map[string]ResultProcessorFunc
	mutex      sync.RWMutex
	logger     *logrus.Logger
}

// ResultProcessorFunc processes scan results from a specific scanner
type ResultProcessorFunc func(ctx context.Context, results *ScanResults) ([]*entity.Vulnerability, error)

// AssetServiceInterface defines the interface for asset service integration
type AssetServiceInterface interface {
	GetAssetsByFilter(ctx context.Context, tenantID uuid.UUID, filter AssetFilter) ([]Asset, error)
	GetAssetByID(ctx context.Context, tenantID, assetID uuid.UUID) (*Asset, error)
	UpdateAssetScanStatus(ctx context.Context, assetID uuid.UUID, scanID uuid.UUID, status string) error
	GetAssetCriticality(ctx context.Context, assetID uuid.UUID) (string, error)
}

// NotificationServiceInterface defines the interface for notifications
type NotificationServiceInterface interface {
	SendScanStarted(ctx context.Context, scan *entity.Scan) error
	SendScanCompleted(ctx context.Context, scan *entity.Scan, results *ScanResults) error
	SendScanFailed(ctx context.Context, scan *entity.Scan, error error) error
	SendHighRiskVulnerabilities(ctx context.Context, vulnerabilities []*entity.Vulnerability) error
}

// Asset represents asset information from asset service
type Asset struct {
	ID               uuid.UUID         `json:"id"`
	Name             string            `json:"name"`
	Type             string            `json:"type"`
	Criticality      string            `json:"criticality"`
	IPAddresses      []string          `json:"ip_addresses"`
	HostNames        []string          `json:"host_names"`
	NetworkSegment   string            `json:"network_segment"`
	BusinessFunction string            `json:"business_function"`
	Owner            string            `json:"owner"`
	LastScanDate     *time.Time        `json:"last_scan_date,omitempty"`
	ScanFrequency    string            `json:"scan_frequency"`
	Tags             map[string]string `json:"tags"`
}

// AssetFilter represents asset filtering criteria
type AssetFilter struct {
	AssetTypes        []string          `json:"asset_types,omitempty"`
	Criticalities     []string          `json:"criticalities,omitempty"`
	BusinessFunctions []string          `json:"business_functions,omitempty"`
	NetworkSegments   []string          `json:"network_segments,omitempty"`
	Tags              map[string]string `json:"tags,omitempty"`
	LastScanBefore    *time.Time        `json:"last_scan_before,omitempty"`
	ScanFrequency     []string          `json:"scan_frequency,omitempty"`
}

// NewScanOrchestrator creates a new scan orchestrator
func NewScanOrchestrator(
	logger *logrus.Logger,
	config OrchestratorConfig,
	assetService AssetServiceInterface,
	notificationService NotificationServiceInterface,
) *ScanOrchestrator {
	orchestrator := &ScanOrchestrator{
		logger:              logger,
		scanners:            make(map[string]VulnerabilityScanner),
		queue:               NewScanQueue(config.MaxQueueSize),
		scheduler:           NewScanScheduler(logger),
		executor:            NewScanExecutor(config.MaxConcurrentScans, logger),
		resultProcessor:     NewResultProcessor(logger),
		assetService:        assetService,
		notificationService: notificationService,
		config:              config,
		metrics: &OrchestratorMetrics{
			ScannerUtilization: make(map[string]float64),
			LastMetricsUpdate:  time.Now().UTC(),
		},
	}

	// Start background processes
	go orchestrator.startQueueProcessor()
	go orchestrator.startMetricsCollector()

	if config.EnableAutoScheduling {
		go orchestrator.scheduler.Start()
	}

	logger.WithFields(logrus.Fields{
		"component":       "scan_orchestrator",
		"max_concurrent":  config.MaxConcurrentScans,
		"max_queue_size":  config.MaxQueueSize,
		"auto_scheduling": config.EnableAutoScheduling,
	}).Info("Scan orchestrator initialized")

	return orchestrator
}

// RegisterScanner registers a vulnerability scanner
func (o *ScanOrchestrator) RegisterScanner(scanner VulnerabilityScanner) error {
	o.mutex.Lock()
	defer o.mutex.Unlock()

	name := scanner.GetName()
	if _, exists := o.scanners[name]; exists {
		return fmt.Errorf("scanner %s already registered", name)
	}

	o.scanners[name] = scanner
	o.metrics.ScannerUtilization[name] = 0.0

	o.logger.WithFields(logrus.Fields{
		"scanner": name,
		"type":    scanner.GetType(),
		"version": scanner.GetVersion(),
	}).Info("Scanner registered")

	return nil
}

// GetRegisteredScanners returns all registered scanners
func (o *ScanOrchestrator) GetRegisteredScanners() map[string]VulnerabilityScanner {
	o.mutex.RLock()
	defer o.mutex.RUnlock()

	scanners := make(map[string]VulnerabilityScanner)
	for name, scanner := range o.scanners {
		scanners[name] = scanner
	}
	return scanners
}

// SubmitScan submits a scan for execution
func (o *ScanOrchestrator) SubmitScan(ctx context.Context, scan *entity.Scan) (*QueueItem, error) {
	logger := o.logger.WithFields(logrus.Fields{
		"operation": "submit_scan",
		"scan_id":   scan.ID,
		"scan_type": scan.ScanType,
		"scanner":   scan.Scanner,
	})

	logger.Info("Submitting scan for execution")

	// Validate scan
	if err := scan.Validate(); err != nil {
		logger.WithError(err).Error("Scan validation failed")
		return nil, fmt.Errorf("scan validation failed: %w", err)
	}

	// Check if scanner is registered
	o.mutex.RLock()
	scanner, exists := o.scanners[scan.Scanner]
	o.mutex.RUnlock()

	if !exists {
		return nil, fmt.Errorf("scanner %s not registered", scan.Scanner)
	}

	// Validate scanner configuration
	if err := scanner.Validate(ctx, scan.ScannerConfig); err != nil {
		logger.WithError(err).Error("Scanner configuration validation failed")
		return nil, fmt.Errorf("scanner configuration validation failed: %w", err)
	}

	// Resolve assets if using asset filters
	if err := o.resolveAssets(ctx, scan); err != nil {
		logger.WithError(err).Error("Asset resolution failed")
		return nil, fmt.Errorf("asset resolution failed: %w", err)
	}

	// Create queue item
	queueItem := &QueueItem{
		ID:           uuid.New(),
		Scan:         scan,
		Priority:     scan.GetPriorityScore(),
		QueuedAt:     time.Now().UTC(),
		ScheduledAt:  time.Now().UTC(),
		RetryCount:   0,
		MaxRetries:   3,
		Dependencies: []uuid.UUID{},
		Tags:         make(map[string]string),
	}

	// Add to queue
	if err := o.queue.Enqueue(queueItem); err != nil {
		logger.WithError(err).Error("Failed to enqueue scan")
		return nil, fmt.Errorf("failed to enqueue scan: %w", err)
	}

	// Update metrics
	o.metrics.TotalScansRequested++
	o.metrics.CurrentQueueSize = o.queue.Size()

	// Send notification
	if o.notificationService != nil {
		go func() {
			if err := o.notificationService.SendScanStarted(context.Background(), scan); err != nil {
				o.logger.WithError(err).Warn("Failed to send scan started notification")
			}
		}()
	}

	logger.WithField("queue_item_id", queueItem.ID).Info("Scan submitted successfully")
	return queueItem, nil
}

// GetScanStatus returns the status of a scan
func (o *ScanOrchestrator) GetScanStatus(ctx context.Context, scanID uuid.UUID) (*ScanStatusInfo, error) {
	// Check if scan is in queue
	if queueItem := o.queue.GetItem(scanID); queueItem != nil {
		return &ScanStatusInfo{
			ScanID:     scanID,
			Status:     entity.ScanStatusQueued,
			QueuedAt:   &queueItem.QueuedAt,
			Position:   o.queue.GetPosition(scanID),
			RetryCount: queueItem.RetryCount,
		}, nil
	}

	// Check if scan is executing
	o.executor.mutex.RLock()
	session, exists := o.executor.activeSessions[scanID]
	o.executor.mutex.RUnlock()

	if exists {
		return &ScanStatusInfo{
			ScanID:      scanID,
			Status:      session.Status,
			StartedAt:   &session.StartedAt,
			Progress:    session.Progress,
			Scanner:     session.ScannerName,
			ExecutionID: &session.ID,
		}, nil
	}

	return nil, fmt.Errorf("scan %s not found", scanID)
}

// CancelScan cancels a scan
func (o *ScanOrchestrator) CancelScan(ctx context.Context, scanID uuid.UUID, reason string) error {
	logger := o.logger.WithFields(logrus.Fields{
		"operation": "cancel_scan",
		"scan_id":   scanID,
		"reason":    reason,
	})

	logger.Info("Cancelling scan")

	// Try to remove from queue first
	if removed := o.queue.Remove(scanID); removed {
		logger.Info("Scan removed from queue")
		o.metrics.TotalScansCancelled++
		return nil
	}

	// Try to cancel active execution
	o.executor.mutex.RLock()
	session, exists := o.executor.activeSessions[scanID]
	o.executor.mutex.RUnlock()

	if exists {
		// Cancel the context
		session.CancelFunc()

		// Try to cancel via scanner
		if err := session.Scanner.CancelScan(ctx, session.ID); err != nil {
			logger.WithError(err).Warn("Scanner cancellation failed")
		}

		// Update status
		session.Status = entity.ScanStatusCancelled
		session.Scan.Cancel(reason)

		o.metrics.TotalScansCancelled++
		logger.Info("Scan cancelled successfully")
		return nil
	}

	return fmt.Errorf("scan %s not found or already completed", scanID)
}

// GetQueueStatus returns queue status information
func (o *ScanOrchestrator) GetQueueStatus() *QueueStatus {
	return &QueueStatus{
		Size:          o.queue.Size(),
		MaxSize:       o.queue.maxSize,
		ActiveScans:   len(o.executor.activeSessions),
		MaxConcurrent: o.executor.maxConcurrent,
		Metrics:       o.queue.metrics,
		Items:         o.queue.GetItems(),
	}
}

// GetOrchestratorMetrics returns orchestrator metrics
func (o *ScanOrchestrator) GetOrchestratorMetrics() *OrchestratorMetrics {
	o.mutex.RLock()
	defer o.mutex.RUnlock()

	// Update current values
	o.metrics.CurrentActiveScans = len(o.executor.activeSessions)
	o.metrics.CurrentQueueSize = o.queue.Size()
	o.metrics.LastMetricsUpdate = time.Now().UTC()

	return o.metrics
}

// GetScannerHealth returns health status of all scanners
func (o *ScanOrchestrator) GetScannerHealth(ctx context.Context) (map[string]*ScannerHealth, error) {
	o.mutex.RLock()
	scanners := make(map[string]VulnerabilityScanner)
	for name, scanner := range o.scanners {
		scanners[name] = scanner
	}
	o.mutex.RUnlock()

	health := make(map[string]*ScannerHealth)
	for name, scanner := range scanners {
		if scannerHealth, err := scanner.GetHealth(ctx); err == nil {
			health[name] = scannerHealth
		} else {
			health[name] = &ScannerHealth{
				Status: HealthStatusUnhealthy,
				Issues: []HealthIssue{{
					Code:      "HEALTH_CHECK_FAILED",
					Message:   err.Error(),
					Severity:  "error",
					Component: name,
					Timestamp: time.Now().UTC(),
				}},
			}
		}
	}

	return health, nil
}

// Private methods

func (o *ScanOrchestrator) resolveAssets(ctx context.Context, scan *entity.Scan) error {
	if o.assetService == nil {
		return nil // No asset service available
	}

	// If asset filters are specified, resolve them to asset IDs
	if len(scan.AssetFilters.AssetTypes) > 0 ||
		len(scan.AssetFilters.Criticalities) > 0 ||
		len(scan.AssetFilters.Tags) > 0 {

		filter := AssetFilter{
			AssetTypes:        scan.AssetFilters.AssetTypes,
			Criticalities:     scan.AssetFilters.Criticalities,
			BusinessFunctions: scan.AssetFilters.BusinessFunctions,
			NetworkSegments:   scan.AssetFilters.NetworkSegments,
			Tags:              scan.AssetFilters.Tags,
			LastScanBefore:    scan.AssetFilters.LastScanBefore,
		}

		assets, err := o.assetService.GetAssetsByFilter(ctx, scan.TenantID, filter)
		if err != nil {
			return fmt.Errorf("failed to resolve assets: %w", err)
		}

		// Add resolved asset IDs
		for _, asset := range assets {
			scan.AssetIDs = append(scan.AssetIDs, asset.ID)
		}

		o.logger.WithFields(logrus.Fields{
			"scan_id":      scan.ID,
			"assets_found": len(assets),
		}).Debug("Assets resolved from filters")
	}

	return nil
}

func (o *ScanOrchestrator) startQueueProcessor() {
	ticker := time.NewTicker(o.config.QueueProcessingInterval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			o.processQueue()
		}
	}
}

func (o *ScanOrchestrator) processQueue() {
	// Check if we can start more scans
	if len(o.executor.activeSessions) >= o.executor.maxConcurrent {
		return
	}

	// Get next item from queue
	item := o.queue.Dequeue()
	if item == nil {
		return
	}

	// Start scan execution
	go o.executeScan(item)
}

func (o *ScanOrchestrator) executeScan(item *QueueItem) {
	logger := o.logger.WithFields(logrus.Fields{
		"scan_id":       item.Scan.ID,
		"queue_item_id": item.ID,
		"scanner":       item.Scan.Scanner,
	})

	logger.Info("Starting scan execution")

	// Get scanner
	o.mutex.RLock()
	scanner, exists := o.scanners[item.Scan.Scanner]
	o.mutex.RUnlock()

	if !exists {
		logger.Error("Scanner not found")
		o.handleScanError(item, fmt.Errorf("scanner %s not found", item.Scan.Scanner))
		return
	}

	// Create execution context
	ctx, cancel := context.WithTimeout(context.Background(), o.config.ScanTimeout)
	defer cancel()

	// Create execution session
	session := &ExecutionSession{
		ID:            uuid.New(),
		ScanID:        item.Scan.ID,
		Scanner:       scanner,
		ScannerName:   item.Scan.Scanner,
		Status:        entity.ScanStatusRunning,
		StartedAt:     time.Now().UTC(),
		Context:       ctx,
		CancelFunc:    cancel,
		ResultChannel: make(chan *ScanResults, 1),
		ErrorChannel:  make(chan error, 1),
		Metadata:      make(map[string]interface{}),
	}

	// Register session
	o.executor.mutex.Lock()
	o.executor.activeSessions[item.Scan.ID] = session
	o.executor.mutex.Unlock()

	// Update scan status
	item.Scan.Start()

	// Start scan
	execution, err := scanner.StartScan(ctx, item.Scan)
	if err != nil {
		logger.WithError(err).Error("Failed to start scan")
		o.handleScanError(item, err)
		return
	}

	// Monitor scan progress
	go o.monitorScanProgress(ctx, session, execution)

	// Wait for completion
	select {
	case results := <-session.ResultChannel:
		o.handleScanCompletion(item, session, results)
	case err := <-session.ErrorChannel:
		o.handleScanError(item, err)
	case <-ctx.Done():
		o.handleScanTimeout(item, session)
	}

	// Clean up session
	o.executor.mutex.Lock()
	delete(o.executor.activeSessions, item.Scan.ID)
	o.executor.mutex.Unlock()
}

func (o *ScanOrchestrator) monitorScanProgress(ctx context.Context, session *ExecutionSession, execution *ScanExecution) {
	ticker := time.NewTicker(30 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			progress, err := session.Scanner.MonitorScan(ctx, execution.ID)
			if err != nil {
				o.logger.WithError(err).Warn("Failed to monitor scan progress")
				continue
			}

			session.Progress = progress
			session.Status = progress.Status

			// Check if scan completed
			if progress.Status == entity.ScanStatusCompleted {
				results, err := session.Scanner.GetScanResults(ctx, execution.ID)
				if err != nil {
					session.ErrorChannel <- err
				} else {
					session.ResultChannel <- results
				}
				return
			} else if progress.Status == entity.ScanStatusFailed {
				session.ErrorChannel <- fmt.Errorf("scan failed")
				return
			}
		}
	}
}

func (o *ScanOrchestrator) handleScanCompletion(item *QueueItem, session *ExecutionSession, results *ScanResults) {
	logger := o.logger.WithFields(logrus.Fields{
		"scan_id":               item.Scan.ID,
		"vulnerabilities_found": len(results.Vulnerabilities),
	})

	logger.Info("Scan completed successfully")

	// Update scan with results
	item.Scan.Complete()
	item.Scan.Results.VulnerabilitiesFound = len(results.Vulnerabilities)
	item.Scan.Results.RiskScore = results.Summary.RiskScore

	// Update metrics
	o.metrics.TotalScansCompleted++
	o.updateAverageDuration(session.StartedAt)

	// Send completion notification
	if o.notificationService != nil {
		go func() {
			if err := o.notificationService.SendScanCompleted(context.Background(), item.Scan, results); err != nil {
				o.logger.WithError(err).Warn("Failed to send scan completion notification")
			}
		}()
	}

	// Process high-risk vulnerabilities
	o.processHighRiskVulnerabilities(results.Vulnerabilities)
}

func (o *ScanOrchestrator) handleScanError(item *QueueItem, err error) {
	logger := o.logger.WithFields(logrus.Fields{
		"scan_id": item.Scan.ID,
		"error":   err.Error(),
	})

	// Check if we should retry
	if item.RetryCount < item.MaxRetries {
		item.RetryCount++
		logger.WithField("retry_count", item.RetryCount).Info("Retrying scan")

		// Re-queue the scan
		o.queue.Enqueue(item)
		return
	}

	logger.Error("Scan failed permanently")

	// Update scan status
	item.Scan.Fail(err.Error())

	// Update metrics
	o.metrics.TotalScansFailed++

	// Send failure notification
	if o.notificationService != nil {
		go func() {
			if err := o.notificationService.SendScanFailed(context.Background(), item.Scan, err); err != nil {
				o.logger.WithError(err).Warn("Failed to send scan failure notification")
			}
		}()
	}
}

func (o *ScanOrchestrator) handleScanTimeout(item *QueueItem, session *ExecutionSession) {
	logger := o.logger.WithField("scan_id", item.Scan.ID)
	logger.Warn("Scan timed out")

	// Try to cancel via scanner
	if err := session.Scanner.CancelScan(session.Context, session.ID); err != nil {
		logger.WithError(err).Warn("Failed to cancel timed out scan")
	}

	// Handle as error
	o.handleScanError(item, fmt.Errorf("scan timed out after %v", o.config.ScanTimeout))
}

func (o *ScanOrchestrator) processHighRiskVulnerabilities(vulnerabilities []*entity.Vulnerability) {
	var highRisk []*entity.Vulnerability
	for _, vuln := range vulnerabilities {
		if vuln.IsHighRisk() {
			highRisk = append(highRisk, vuln)
		}
	}

	if len(highRisk) > 0 && o.notificationService != nil {
		go func() {
			if err := o.notificationService.SendHighRiskVulnerabilities(context.Background(), highRisk); err != nil {
				o.logger.WithError(err).Warn("Failed to send high-risk vulnerability notification")
			}
		}()
	}
}

func (o *ScanOrchestrator) updateAverageDuration(startTime time.Time) {
	duration := time.Since(startTime)
	if o.metrics.AverageScanDuration == 0 {
		o.metrics.AverageScanDuration = duration
	} else {
		// Simple moving average
		o.metrics.AverageScanDuration = (o.metrics.AverageScanDuration + duration) / 2
	}
}

func (o *ScanOrchestrator) startMetricsCollector() {
	ticker := time.NewTicker(o.config.HealthCheckInterval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			o.collectMetrics()
		}
	}
}

func (o *ScanOrchestrator) collectMetrics() {
	// Update scanner utilization
	o.mutex.RLock()
	for name := range o.scanners {
		// Calculate utilization based on active scans
		// This is a simplified calculation
		utilization := 0.0
		for _, session := range o.executor.activeSessions {
			if session.ScannerName == name {
				utilization += 1.0
			}
		}
		o.metrics.ScannerUtilization[name] = utilization
	}
	o.mutex.RUnlock()

	// Calculate success rate
	total := o.metrics.TotalScansCompleted + o.metrics.TotalScansFailed
	if total > 0 {
		o.metrics.SuccessRate = float64(o.metrics.TotalScansCompleted) / float64(total) * 100
	}

	// Calculate throughput (scans per hour)
	if o.metrics.LastMetricsUpdate.Before(time.Now().Add(-time.Hour)) {
		o.metrics.ThroughputPerHour = float64(o.metrics.TotalScansCompleted)
	}
}

// Supporting types

type ScanStatusInfo struct {
	ScanID      uuid.UUID         `json:"scan_id"`
	Status      entity.ScanStatus `json:"status"`
	QueuedAt    *time.Time        `json:"queued_at,omitempty"`
	StartedAt   *time.Time        `json:"started_at,omitempty"`
	Position    int               `json:"position,omitempty"`
	Progress    *ScanProgress     `json:"progress,omitempty"`
	Scanner     string            `json:"scanner,omitempty"`
	ExecutionID *uuid.UUID        `json:"execution_id,omitempty"`
	RetryCount  int               `json:"retry_count"`
}

type QueueStatus struct {
	Size          int           `json:"size"`
	MaxSize       int           `json:"max_size"`
	ActiveScans   int           `json:"active_scans"`
	MaxConcurrent int           `json:"max_concurrent"`
	Metrics       *QueueMetrics `json:"metrics"`
	Items         []*QueueItem  `json:"items"`
}

// Queue implementation

func NewScanQueue(maxSize int) *ScanQueue {
	return &ScanQueue{
		items:   make([]*QueueItem, 0),
		maxSize: maxSize,
		metrics: &QueueMetrics{},
	}
}

func (q *ScanQueue) Enqueue(item *QueueItem) error {
	q.mutex.Lock()
	defer q.mutex.Unlock()

	if len(q.items) >= q.maxSize {
		q.metrics.ItemsDropped++
		return fmt.Errorf("queue is full")
	}

	q.items = append(q.items, item)
	q.metrics.ItemsQueued++

	// Sort by priority (highest first)
	sort.Slice(q.items, func(i, j int) bool {
		return q.items[i].Priority > q.items[j].Priority
	})

	return nil
}

func (q *ScanQueue) Dequeue() *QueueItem {
	q.mutex.Lock()
	defer q.mutex.Unlock()

	if len(q.items) == 0 {
		return nil
	}

	item := q.items[0]
	q.items = q.items[1:]
	q.metrics.ItemsProcessed++

	return item
}

func (q *ScanQueue) Size() int {
	q.mutex.RLock()
	defer q.mutex.RUnlock()
	return len(q.items)
}

func (q *ScanQueue) GetItem(scanID uuid.UUID) *QueueItem {
	q.mutex.RLock()
	defer q.mutex.RUnlock()

	for _, item := range q.items {
		if item.Scan.ID == scanID {
			return item
		}
	}
	return nil
}

func (q *ScanQueue) Remove(scanID uuid.UUID) bool {
	q.mutex.Lock()
	defer q.mutex.Unlock()

	for i, item := range q.items {
		if item.Scan.ID == scanID {
			q.items = append(q.items[:i], q.items[i+1:]...)
			return true
		}
	}
	return false
}

func (q *ScanQueue) GetPosition(scanID uuid.UUID) int {
	q.mutex.RLock()
	defer q.mutex.RUnlock()

	for i, item := range q.items {
		if item.Scan.ID == scanID {
			return i + 1
		}
	}
	return -1
}

func (q *ScanQueue) GetItems() []*QueueItem {
	q.mutex.RLock()
	defer q.mutex.RUnlock()

	items := make([]*QueueItem, len(q.items))
	copy(items, q.items)
	return items
}

// Additional implementations for scheduler and executor

func NewScanScheduler(logger *logrus.Logger) *ScanScheduler {
	return &ScanScheduler{
		schedules: make(map[uuid.UUID]*ScheduleEntry),
		logger:    logger,
		stopChan:  make(chan bool),
	}
}

func (s *ScanScheduler) Start() {
	s.mutex.Lock()
	if s.isRunning {
		s.mutex.Unlock()
		return
	}
	s.isRunning = true
	s.ticker = time.NewTicker(time.Minute)
	s.mutex.Unlock()

	for {
		select {
		case <-s.ticker.C:
			s.checkSchedules()
		case <-s.stopChan:
			s.ticker.Stop()
			return
		}
	}
}

func (s *ScanScheduler) Stop() {
	s.mutex.Lock()
	defer s.mutex.Unlock()
	if s.isRunning {
		s.stopChan <- true
		s.isRunning = false
	}
}

func (s *ScanScheduler) checkSchedules() {
	// Implementation for checking and triggering scheduled scans
	// This would parse cron expressions and trigger scans
}

func NewScanExecutor(maxConcurrent int, logger *logrus.Logger) *ScanExecutor {
	return &ScanExecutor{
		activeSessions: make(map[uuid.UUID]*ExecutionSession),
		maxConcurrent:  maxConcurrent,
		logger:         logger,
	}
}

func NewResultProcessor(logger *logrus.Logger) *ResultProcessor {
	return &ResultProcessor{
		processors: make(map[string]ResultProcessorFunc),
		logger:     logger,
	}
}
